{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ccf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Step 0: Download and Preprocess\n",
      "===============================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/sybig/home/tmu/TUGDA/data/GDSCDA_fpkm_AUC_all_drugs.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m31\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load the full GDSC dataset (FPKM + AUC values for all drugs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m gdsc_dataset = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/sybig/home/tmu/TUGDA/data/GDSCDA_fpkm_AUC_all_drugs.zip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Extract gene and drug columns:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - First 1780 columns correspond to gene expression data\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# - Remaining columns represent drug AUC values\u001b[39;00m\n\u001b[32m     25\u001b[39m gene_list = gdsc_dataset.columns[\u001b[32m0\u001b[39m:\u001b[32m1780\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/common.py:794\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# ZIP Compression\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m compression == \u001b[33m\"\u001b[39m\u001b[33mzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_BytesZipFile\" has incompatible type\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[39;00m\n\u001b[32m    793\u001b[39m     \u001b[38;5;66;03m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     handle = \u001b[43m_BytesZipFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompression_args\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m handle.buffer.mode == \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    798\u001b[39m         handles.append(handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/pandas/io/common.py:1037\u001b[39m, in \u001b[36m_BytesZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, archive_name, **kwargs)\u001b[39m\n\u001b[32m   1034\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m, zipfile.ZIP_DEFLATED)\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type \"ZipFile\",\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m# base class \"_BufferedWriter\" defined the type as \"BytesIO\")\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m \u001b[38;5;28mself\u001b[39m.buffer: zipfile.ZipFile = \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/zipfile/__init__.py:1331\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1333\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/sybig/home/tmu/TUGDA/data/GDSCDA_fpkm_AUC_all_drugs.zip'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # read Dataframe \n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data # for creating graph-based datasets\n",
    "import os # for file and directory operations \n",
    "import numpy as np # for numerical computations with arrays \n",
    "import gseapy as gp # for retrieving pathway information\n",
    "\n",
    "\n",
    "# Creating a Custom Dataset in Pytorch Geometric \n",
    "\n",
    "# -----------------------------------\n",
    "# Step 0: Load and Prepare DataFrames\n",
    "# -----------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*31)\n",
    "print(\"Step 0: Download and Preprocess\")\n",
    "print(\"=\"*31)\n",
    "\n",
    "# Load the full GDSC dataset (FPKM + AUC values for all drugs)\n",
    "gdsc_dataset = pd.read_csv('/sybig/home/tmu/TUGDA/data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "# gdsc_dataset = pd.read_csv('/Users/tm03/Desktop/TUGDA_1/data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "# Extract gene and drug columns:\n",
    "# - First 1780 columns correspond to gene expression data\n",
    "# - Remaining columns represent drug AUC values\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:] \n",
    "\n",
    "# Retrieve KEGG pathways using gseapy\n",
    "kegg_gmt = gp.parser.get_library('KEGG_2021_Human', organism='Human', min_size=3, max_size=2000)\n",
    "pathway_list = list(kegg_gmt.keys())\n",
    "\n",
    "# Gene Expression Data (FPKM values)\n",
    "expression_data = gdsc_dataset.iloc[:, :1780]\n",
    "\n",
    "# Response Data: Combine log_IC50 values from 3-fold cross-validation test sets\n",
    "response_1 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k1.csv\", index_col=0)\n",
    "response_2 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k2.csv\", index_col=0)\n",
    "response_3 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k3.csv\", index_col=0)\n",
    "\n",
    "# response_1 = pd.read_csv(\"/Users/tm03/Desktop/TUGDA_1/data/cl_y_test_o_k1.csv\", index_col=0)\n",
    "# response_2 = pd.read_csv(\"/Users/tm03/Desktop/TUGDA_1/data/cl_y_test_o_k2.csv\", index_col=0)\n",
    "# response_3 = pd.read_csv(\"/Users/tm03/Desktop/TUGDA_1/data/cl_y_test_o_k3.csv\", index_col=0)\n",
    "# response_data = pd.concat([response_1, response_2, response_3], axis=0, ignore_index=False)\n",
    "\n",
    "# Sort both datasets by index to ensure alignment\n",
    "expression_data = expression_data.sort_index()\n",
    "response_data = response_data.sort_index()\n",
    "\n",
    "# Remove duplicate indices (keep first occurrence) to avoid conflicts during merging\n",
    "expression_data = expression_data[~expression_data.index.duplicated(keep='first')]\n",
    "labels_df = response_data[~response_data.index.duplicated(keep='first')] \n",
    "\n",
    "print(\"\\n\" + \"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615347f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der NaN-Werte insgesamt: 0\n",
      "Anzahl der NaN-Werte insgesamt: 24340\n"
     ]
    }
   ],
   "source": [
    "nan_count_total = expression_data.isna().sum().sum()\n",
    "print(f\"Anzahl der NaN-Werte insgesamt: {nan_count_total}\")\n",
    "\n",
    "nan_count_total = labels_df.isna().sum().sum()\n",
    "print(f\"Anzahl der NaN-Werte insgesamt: {nan_count_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc76663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Oxaliplatin  Ulixertinib  Fulvestrant  Uprosertib  Dactinomycin  \\\n",
      "22RV1        3.081337     2.803572     4.227332    1.133014     -4.923521   \n",
      "23132-87     4.391634     2.526244     3.697545    0.188871     -4.530814   \n",
      "42-MG-BA     4.129735     3.147708     4.045351    1.682197     -4.231256   \n",
      "5637         4.391109     2.884360     3.535578    4.453362     -4.262836   \n",
      "639-V        3.253815     2.546112     3.550254    4.297324     -4.760262   \n",
      "...               ...          ...          ...         ...           ...   \n",
      "YAPC         5.245318     4.309617     4.761417    5.532699     -2.255783   \n",
      "YH-13        3.945620     3.377953     3.812616    2.172373     -3.734196   \n",
      "YT           0.461330     1.745128     2.024318    4.395238     -5.679264   \n",
      "ZR-75-30     7.154014     6.182055     5.587853    3.443328     -0.055238   \n",
      "huH-1        6.006727     4.090465     4.028713    3.325009     -0.850808   \n",
      "\n",
      "          Docetaxel  Camptothecin  5-Fluorouracil  Afatinib  Taselisib  ...  \\\n",
      "22RV1     -2.941937     -3.142631        2.478859  2.433064   0.972964  ...   \n",
      "23132-87  -5.210999     -3.276945        2.168003  0.652471  -0.320857  ...   \n",
      "42-MG-BA  -4.247072     -2.906706        3.362543  1.723549   0.896352  ...   \n",
      "5637      -5.553314     -2.314530        3.313258 -0.036451   1.822476  ...   \n",
      "639-V     -4.869332     -5.115998        4.410077  1.728613   2.683493  ...   \n",
      "...             ...           ...             ...       ...        ...  ...   \n",
      "YAPC       1.326340     -1.756217        5.638735  2.535585   2.528939  ...   \n",
      "YH-13     -5.772570     -1.462977        4.882430  2.042245   0.323799  ...   \n",
      "YT        -4.989452     -4.794935        0.759572  1.746264   0.800917  ...   \n",
      "ZR-75-30   1.329350      2.638266        7.142114 -1.255299   2.513473  ...   \n",
      "huH-1     -2.344459     -1.004498        6.373681  2.990538   3.620741  ...   \n",
      "\n",
      "          BMS-754807   BI-2536  Cetuximab  Doxorubicin  Etoposide  Bleomycin  \\\n",
      "22RV1            NaN       NaN   6.683803    -2.500206   0.246500   1.938966   \n",
      "23132-87         NaN       NaN   6.296225    -3.525237  -0.488340  -0.510824   \n",
      "42-MG-BA         NaN       NaN   6.367809    -2.129720   2.420287   2.128587   \n",
      "5637             NaN       NaN   5.181353    -3.009819  -0.207230  -1.679493   \n",
      "639-V            NaN       NaN   5.332501    -3.358160   0.188715  -1.227591   \n",
      "...              ...       ...        ...          ...        ...        ...   \n",
      "YAPC             NaN       NaN   7.164002     0.239293   3.578462   6.961423   \n",
      "YH-13            NaN       NaN        NaN    -0.721920   4.373041   4.716770   \n",
      "YT               NaN       NaN   6.334848    -0.093297   2.301408   0.431449   \n",
      "ZR-75-30         NaN  3.724833        NaN     1.044187        NaN        NaN   \n",
      "huH-1            NaN       NaN   6.136704     0.620599   5.166316   5.241475   \n",
      "\n",
      "          Bicalutamide  Bleomycin (50 uM)  Pemetrexed  AICA Ribonucleotide  \n",
      "22RV1         3.673067           4.031290    1.456603             8.277725  \n",
      "23132-87      3.465794           0.525526    0.506745             8.505100  \n",
      "42-MG-BA      3.524764          -0.450167    2.582966             7.562607  \n",
      "5637          3.206796          -0.494399    0.572228             8.011649  \n",
      "639-V         3.151029           0.061915    2.500569             7.448760  \n",
      "...                ...                ...         ...                  ...  \n",
      "YAPC          3.812996           2.382738    1.303840             9.325474  \n",
      "YH-13         3.498026           1.017930    3.711630             9.043458  \n",
      "YT            3.118420           4.505120    2.112467             8.023384  \n",
      "ZR-75-30           NaN           7.171414         NaN             8.513561  \n",
      "huH-1         4.172252           3.268635    4.467953             7.678649  \n",
      "\n",
      "[804 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa52a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Step 1: GNN Dataset & Quick Check\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Name: Camptothecin\n",
      "Cell Line Name: 23132-87\n",
      "Edge Index (COO format):\n",
      "tensor([[   0,    0],\n",
      "        [   0,    4],\n",
      "        [   0,    6],\n",
      "        ...,\n",
      "        [1825, 1825],\n",
      "        [1826,  440],\n",
      "        [1826, 1826]])\n",
      "torch.Size([52461, 2])\n",
      "\n",
      "Node Features (x):\n",
      "tensor([[-1.5698,  1.0000,  0.0000],\n",
      "        [ 0.6585,  1.0000,  0.0000],\n",
      "        [-0.5985,  1.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2597,  1.0000,  0.0000],\n",
      "        [-0.4080,  1.0000,  0.0000],\n",
      "        [ 0.1387,  1.0000,  0.0000]])\n",
      "torch.Size([1827, 3])\n",
      "Index 0: ABL1 → Feature: -1.5698\n",
      "Index 1: ACVR1B → Feature: 0.6585\n",
      "Index 2: ADORA1 → Feature: -0.5985\n",
      "Index 3: AKT1 → Feature: 0.2557\n",
      "Index 4: AR → Feature: -0.4920\n",
      "Index 5: ATF4 → Feature: 0.8800\n",
      "Index 6: ATM → Feature: 0.0323\n",
      "Index 7: ATR → Feature: 0.2790\n",
      "Index 8: AURKA → Feature: 1.0195\n",
      "Index 9: BAX → Feature: -0.2325\n",
      "\n",
      "Label (y):\n",
      "tensor([-3.2769])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "### GNN Dataset ###\n",
    "###################\n",
    "\n",
    "print(\"\\n\" + \"=\"*33)\n",
    "print(\"Step 1: GNN Dataset & Quick Check\")\n",
    "print(\"=\"*33)\n",
    "\n",
    "class DrugNetworkDataset(Dataset):\n",
    "    def __init__(self, root, drug_list, gene_list, pathway_list, labels_df, expression_data, transform=None, pre_transform=None):\n",
    "\n",
    "        \"\"\"\n",
    "        A custom PyTorch Geometric Dataset for drug-cell line interaction graphs.\n",
    "\n",
    "        Parameters:\n",
    "            - root = Where the datase4t should be stored. This folder is split \n",
    "            - into raw_dir (downloaded datset) and processed_dir (processed data)\n",
    "\n",
    "            - drug_list: List of drugs (tasks) (200)\n",
    "            - gene_list: Lisz of genes sample (1780)\n",
    "            - pathway_list: List of pathways from KEGG \n",
    "            - labels_df: response data (log_IC50)\n",
    "            - expression_data: Gene expression values (preprocessed according to Mourragui et al. (2020): library-size using TMM, log-transformed, gene-level-mean-centering and standardization)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define all files that the dataset needs\n",
    "        self.drug_list = drug_list\n",
    "        self.gene_list = gene_list\n",
    "        self.pathway_list = pathway_list\n",
    "        self.labels_df = labels_df\n",
    "        self.expression_data = expression_data\n",
    "\n",
    "        # Get all combination of Drug + Cell Line\n",
    "        self.samples = [\n",
    "            (drug, cell_line) \n",
    "            for drug in self.drug_list \n",
    "            for cell_line in self.labels_df.index\n",
    "        ]\n",
    "\n",
    "        # Define custom raw_dir\n",
    "        self.custom_raw_dir = os.path.join(root, 'drug_matrices_csv')\n",
    "\n",
    "        super(DrugNetworkDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If these files exist in raw_dir, the download is not triggered \"\"\"\n",
    "        return [f\"{drug}_matrix.csv\" for drug in self.drug_list]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return self.custom_raw_dir\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files exist in processed_dir, processing is skipped \"\"\"\n",
    "        return ['placeholder.pt'] # not implemented \n",
    "        #  return [f'data_{i}.pt' for i in range(len(self.samples))]\n",
    "\n",
    "    def download(self):\n",
    "        pass \n",
    "\n",
    "    def _load_adjacency_matrix(self, drug):\n",
    "        \"\"\" Loads the adjacency matrix for every drug \"\"\"\n",
    "        csv_path = os.path.join(self.raw_dir, f\"{drug}_matrix.csv\")\n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        return df\n",
    "\n",
    "    def _get_node_features(self, nodes, cell_line):\n",
    "        x = []\n",
    "        for node in nodes:\n",
    "            if node in self.gene_list:\n",
    "                 # If the node is a gene, get its expression value for the given cell line and type [expr, is_gene, is_pathway]\n",
    "                expr_value = self.expression_data.loc[cell_line, node]\n",
    "                x.append([float(expr_value), 1.0, 0.0])  # [expr, is_gene, is_pathway]\n",
    "            elif node in self.pathway_list:\n",
    "                # If the node is a pathway, use a default feature value of 0.0\n",
    "                x.append([0.0, 0.0, 1.0])  # [expr_dummy, is_gene, is_pathway]\n",
    "            else:\n",
    "                # Unknown node\n",
    "                x.append([0.0, 0.0, 0.0])  # [expr_dummy, is_gene, is_pathway]\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        ''' Code for just Gene expression value: \n",
    "        x = []\n",
    "        for node in nodes:\n",
    "             # If the node is a gene, get its expression value for the given cell line\n",
    "            if node in self.gene_list:\n",
    "                expr_value = self.expression_data.loc[cell_line, node]\n",
    "                x.append([float(expr_value)]) # Append as a single-element list \n",
    "            elif node in self.pathway_list:\n",
    "                # If the node is a pathway, use a default feature value of 0.0\n",
    "                x.append([0.0])\n",
    "            else:\n",
    "                x.append([0.0])\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "        ''' \n",
    "    \n",
    "    # Qucik check fuction for debugging\n",
    "    def get_node_name_by_index(self, idx, node_index):\n",
    "        \"\"\" Returns the name of the node at the specified index for the given sample \"\"\"\n",
    "        drug, cell_line = self.samples[idx]\n",
    "        \n",
    "        adj_df = self._load_adjacency_matrix(drug)\n",
    "        nodes = adj_df.columns.tolist()\n",
    "        \n",
    "        if node_index < len(nodes):\n",
    "            return nodes[node_index]\n",
    "        else:\n",
    "            raise IndexError(f\"Node index {node_index} out of range for this graph.\")\n",
    "\n",
    "    def len(self):\n",
    "        \"\"\" \n",
    "        Returns the total number of samples (drug-cell line combinations)\n",
    "        useful for classes such as datasets for machine learning so that they are compatible with len() and for loops \n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"\n",
    "        In standard PyG datasets, get() loads preprocessed data saved via process(); \n",
    "        here the preprocessing step is skipped and construct each drug–cell line graph directly in get(), \n",
    "        as preprocessing wouldn't reduce computation time\n",
    "        \"\"\"\n",
    "        drug, cell_line = self.samples[idx]\n",
    "\n",
    "        # Load adjacency matric for each drug \n",
    "        adj_df = self._load_adjacency_matrix(drug)\n",
    "        nodes = adj_df.columns.tolist()\n",
    "        adj_matrix = adj_df.values\n",
    "        # Build edge_index in COO format\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(adj_matrix)), dtype=torch.long)\n",
    "        # Get node_features\n",
    "        x = self._get_node_features(nodes, cell_line)\n",
    "        # Get label info (log_IC50 value for each drug-cell line combination)\n",
    "        # y = torch.tensor([self.labels_df.loc[cell_line, drug]], dtype=torch.float)\n",
    "\n",
    "        label_value = self.labels_df.loc[cell_line, drug]\n",
    "        if pd.isna(label_value):\n",
    "            raise IndexError(f\"Label is NaN for {drug} + {cell_line}. Skipping.\")\n",
    "        else:\n",
    "            y = torch.tensor([float(label_value)], dtype=torch.float)\n",
    "\n",
    "        # Create data object\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, drug=drug, cell_line=cell_line)\n",
    "\n",
    "        # Attach node names for debugging purposes\n",
    "        data.nodes = nodes\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def process(self):\n",
    "        pass # Skip right now, can be implemented to store the output files in your\n",
    "        '''\n",
    "        for i in range(len(self.samples)):\n",
    "            data = self.get(i)\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "        '''\n",
    "\n",
    "# Run class\n",
    "dataset = DrugNetworkDataset(\n",
    "    root=\"./results/Network/\",\n",
    "    drug_list=drug_list,\n",
    "    gene_list=gene_list,\n",
    "    pathway_list=pathway_list,\n",
    "    labels_df=labels_df, \n",
    "    expression_data=expression_data\n",
    ")\n",
    "\n",
    "# Qucik Ckeck with results\n",
    "data = dataset[1] # graph-base representations of drug-cell line pairs (200 * 1780)\n",
    "\n",
    "print(\"Drug Name:\", data.drug)\n",
    "print(\"Cell Line Name:\", data.cell_line)\n",
    "\n",
    "print(\"Edge Index (COO format):\") # tensor([a,b], [c,d]): node a is conntected to node b and node c is conntected to node d\n",
    "print(data.edge_index.t())\n",
    "print(data.edge_index.t().shape) # Tensor of shape [num_edges, 2]\n",
    "\n",
    "print(\"\\nNode Features (x):\") # Gene expression values of gene_x with Cell line and is_gene, is_pathway\n",
    "print(data.x)\n",
    "print(data.x.shape) # Tensor of shape [num_nodes, num_node_features]\n",
    "\n",
    "nodes = data.nodes\n",
    "\n",
    "for i in range(10):\n",
    "    node_name = nodes[i]\n",
    "    feature_value = data.x[i][0].item()  # Just take the first feature (gene expression)\n",
    "    print(f\"Index {i}: {node_name} → Feature: {feature_value:.4f}\")\n",
    "\n",
    "print(\"\\nLabel (y):\") # log_IC50 value for the drug-cell line combination\n",
    "print(data.y)\n",
    "print(data.y.shape) # Tensor of shape [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2859388",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Label is NaN for BMS-754807 + 23132-87. Skipping.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBMS-754807\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m23132-87\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch_geometric/data/dataset.py:291\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03mpresent).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np.integer))\n\u001b[32m    288\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx.dim() == \u001b[32m0\u001b[39m)\n\u001b[32m    289\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m np.isscalar(idx))):\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     data = data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(data)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mDrugNetworkDataset.get\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    138\u001b[39m label_value = \u001b[38;5;28mself\u001b[39m.labels_df.loc[cell_line, drug]\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(label_value):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel is NaN for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrug\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    142\u001b[39m     y = torch.tensor([\u001b[38;5;28mfloat\u001b[39m(label_value)], dtype=torch.float)\n",
      "\u001b[31mIndexError\u001b[39m: Label is NaN for BMS-754807 + 23132-87. Skipping."
     ]
    }
   ],
   "source": [
    "dataset[dataset.samples.index((\"BMS-754807\", \"23132-87\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd73cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ABL1\n",
      "22RV1    -0.890968\n",
      "23132-87 -1.569799\n",
      "42-MG-BA  0.536505\n",
      "5637      0.873951\n",
      "639-V    -0.708590\n",
      "...            ...\n",
      "YAPC      0.712502\n",
      "YH-13     1.926564\n",
      "YT       -2.203333\n",
      "ZR-75-30 -0.708590\n",
      "huH-1    -0.890968\n",
      "\n",
      "[804 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(expression_data[[\"ABL1\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db522ce8",
   "metadata": {},
   "source": [
    "# First Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6ce27",
   "metadata": {},
   "source": [
    "From the GDSC Data Portal, the gene expression data (RMA normalised expression data for cell-lines, Cell_line_RMA_proc_basalExp.txt) and two annotations (methSampleId_2_cosmicIds.xlsx, Mapping between cell-line COSMIC identifiers and cell-line methylation data identifiers + TableS1E.xlsx, Annotation of cell lines used in the GDSC dataset) was downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330dd67",
   "metadata": {},
   "source": [
    "- three GNN layers with GATConv, Transform, TopK-Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3badf18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1827, 3], edge_index=[2, 52461], y=[1], drug='Camptothecin', cell_line='23132-87', nodes=[1827])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ddc1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GATConv, Linear, TopKPooling, global_mean_pool, global_max_pool\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(GNN, self).__init__()\n",
    "        num_classes = 1 \n",
    "        embedding_size = 256\n",
    "\n",
    "        # three GNN layers\n",
    "        self.conv1 = GATConv(feature_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform1 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool1 = TopKPooling(embedding_size, ratio = 0.8)\n",
    "\n",
    "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform2 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool2 = TopKPooling(embedding_size, ratio = 0.5)\n",
    "\n",
    "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform3 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool3 = TopKPooling(embedding_size, ratio = 0.2)\n",
    "    \n",
    "        # Linear Layers\n",
    "        self.linear1 = Linear(embedding_size*2, 256)\n",
    "        self.linear2 = Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.head_transform1(x)\n",
    "\n",
    "        x, edge_index, _, batch_index, _, _ = self.pool1(x, edge_index, None, batch_index)\n",
    "        x1 = torch.cat([global_max_pool(x, batch_index), global_mean_pool(x, batch_index)], dim=1)\n",
    "\n",
    "        # Second block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.head_transform2(x)\n",
    "\n",
    "        x, edge_index, _, batch_index, _, _ = self.pool1(x, edge_index, None, batch_index)\n",
    "        x2 = torch.cat([global_max_pool(x, batch_index), global_mean_pool(x, batch_index)], dim=1)\n",
    "\n",
    "        # Third block\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.head_transform3(x)\n",
    "\n",
    "        x, edge_index, _, batch_index, _, _ = self.pool1(x, edge_index, None, batch_index)\n",
    "        x3 = torch.cat([global_max_pool(x, batch_index), global_mean_pool(x, batch_index)], dim=1)\n",
    "\n",
    "        # Concat pooled vectors\n",
    "        x = x1 + x2 + x3\n",
    "        \n",
    "        # Output block\n",
    "        x = self.linear1(x).relu()\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14a3a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.032193\n",
      "Epoch: 040, Loss: 2.162785\n",
      "Epoch: 060, Loss: 2.746925\n",
      "Epoch: 080, Loss: 0.493325\n",
      "Epoch: 100, Loss: 1.822758\n",
      "Epoch: 120, Loss: 0.406313\n",
      "Epoch: 140, Loss: 0.111263\n",
      "Epoch: 160, Loss: 1.357738\n",
      "Epoch: 180, Loss: 0.004807\n",
      "Epoch: 200, Loss: 0.022333\n",
      "Prediction: -1.7428950071334839\n",
      "True Value: -3.1426310539245605\n"
     ]
    }
   ],
   "source": [
    "# Batch erstellen (wichtig für Pooling-Layer)\n",
    "data = dataset[0]\n",
    "data.batch = torch.zeros(data.x.shape[0], dtype=torch.long)  # Alle Knoten sind im selben Graph\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Modell, Optimizer & Loss definieren\n",
    "# ----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GNN(feature_size=data.x.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Training Loop\n",
    "# ----------------------------\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    loss = criterion(out, data.y.unsqueeze(0))  # y muss shape [batch_size, 1] haben\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.6f}')\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Evaluation\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(data.x, data.edge_index, data.batch)\n",
    "    print(\"Prediction:\", pred.item())\n",
    "    print(\"True Value:\", data.y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60b2cb",
   "metadata": {},
   "source": [
    "# gcncONV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44c23e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.078836\n",
      "Epoch: 100, Loss: 0.000333\n",
      "Epoch: 150, Loss: 0.000822\n",
      "Epoch: 200, Loss: 0.000166\n",
      "Prediction: -3.1559970378875732\n",
      "True Value: -3.1426310539245605\n"
     ]
    }
   ],
   "source": [
    " # Create a GCN model structure that contains two GCNConv layers relu activation and a dropout rate of 0.5. The model consists of 16 hidden channels.  \n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Modell definieren\n",
    "# ----------------------------\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = torch.dropout(x, p=0.5, train=self.training)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = GCNModel(num_features=3, hidden_channels=16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Training\n",
    "# ----------------------------\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    loss = criterion(out, data.y.unsqueeze(0))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.6f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(data.x, data.edge_index, data.batch)\n",
    "    print(\"Prediction:\", pred.item())\n",
    "    print(\"True Value:\", data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sybig/home/tmu/miniconda3/lib/python3.13/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 9.173643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 3.504404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 2.370892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 2.589767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 2.235986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 2.487135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 2.589848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 2.373393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 2.463561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 2.231457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 2.176494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 2.226722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 2.207750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 2.268195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 2.517760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 2.250463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 2.322152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 2.809301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 2.335249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 2.218212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Loss: 2.332426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Loss: 2.188250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Loss: 2.170479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss: 2.253595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss: 2.287416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Loss: 2.254548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Loss: 2.238691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Loss: 2.375519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Loss: 2.409533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Loss: 2.263764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031, Loss: 2.347821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 032, Loss: 2.233757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 033, Loss: 2.288306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 034, Loss: 2.480765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 035, Loss: 2.269089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Loss: 2.343753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Loss: 2.163118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 038, Loss: 2.840684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 039, Loss: 2.288268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Loss: 2.117000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Loss: 2.294795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 042, Loss: 2.244043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 043, Loss: 2.206588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044, Loss: 2.288073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045, Loss: 2.161090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 046, Loss: 2.147072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 047, Loss: 2.875207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 048, Loss: 2.254188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 049, Loss: 2.100991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 2.178792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:10<00:00,  1.21it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m         preds.extend(out.cpu().numpy())\n\u001b[32m     67\u001b[39m         truths.extend(data.y.cpu().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m rmse = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m r2 = r2_score(truths, preds)\n\u001b[32m     71\u001b[39m pearson = pearsonr(preds, truths)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/inspect.py:3264\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3261\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3262\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/inspect.py:3253\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3243\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3244\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3245\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3250\u001b[39m             ),\n\u001b[32m   3251\u001b[39m         )\n\u001b[32m   3252\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3254\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3255\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Modell definieren\n",
    "# ----------------------------\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = torch.dropout(x, p=0.5, train=self.training)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = GCNModel(num_features=3, hidden_channels=16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. DataLoader erstellen\n",
    "# ----------------------------\n",
    "loader = DataLoader(dataset[:50], batch_size=4, shuffle=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Training Loop mit tqdm\n",
    "# ----------------------------\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch).squeeze()\n",
    "        loss = criterion(out, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d9c78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:11<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Evaluation Metrics\n",
      "========================================\n",
      "R² Score: 0.0472\n",
      "Pearson Correlation: 0.2418\n",
      "\n",
      "Einzelvorhersage:\n",
      "Prediction: -3.2071785926818848\n",
      "True Value: -3.1426310539245605\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5. Evaluation für alle Samples\n",
    "# ----------------------------\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader, desc=\"Evaluating\"):\n",
    "        out = model(data.x, data.edge_index, data.batch).squeeze()\n",
    "        preds.extend(out.cpu().numpy())\n",
    "        truths.extend(data.y.cpu().numpy())\n",
    "\n",
    "r2 = r2_score(truths, preds)\n",
    "pearson = pearsonr(preds, truths)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Evaluation Metrics\")\n",
    "print(\"=\"*40)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Einzelner Graph evaluieren\n",
    "# ----------------------------\n",
    "data_single = dataset[0]\n",
    "pred_single = model(data_single.x, data_single.edge_index, data_single.batch)\n",
    "print(\"\\nEinzelvorhersage:\")\n",
    "print(\"Prediction:\", pred_single.item())\n",
    "print(\"True Value:\", data_single.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd47de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Evaluation Metrics\n",
      "==============================\n",
      "MSE Loss: 2.153636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in loader:  # Alle Daten durchgehen\n",
    "        out = model(data.x, data.edge_index, data.batch).squeeze()\n",
    "        preds.extend(out.cpu().numpy())\n",
    "        truths.extend(data.y.cpu().numpy())\n",
    "\n",
    "# Berechne MSE\n",
    "mse = mean_squared_error(truths, preds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Evaluation Metrics\")\n",
    "print(\"=\"*30)\n",
    "print(f\"MSE Loss: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70859eb8",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51fb5c",
   "metadata": {},
   "source": [
    "Komponenten: \n",
    "- GNNEncoder: Drug-Specific Network Graphen \n",
    "- Aim: tugda_mtl verarbeitet Rohdaten aus Expressionsprofilen --> Drug-spezfische Graphen aus DrugNetworkDataset verwenden, GNN-Encoder als Feature Extractor einbauen \n",
    "\n",
    "- Wir ersetzen:\n",
    "- Die bisherigen nn.Linear-Schichten durch den GNNEncoder. Die Eingabe von X_train durch drug_data (vom Typ Data). Der Output des Encoders wird an die S, A etc. Schichten weitergeleitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a1cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, feature_size=1, embedding_size=512, output_dim=256):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "\n",
    "        # GNN Layers\n",
    "        self.conv1 = GATConv(feature_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform1 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool1 = TopKPooling(embedding_size, ratio=0.8)\n",
    "\n",
    "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform2 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool2 = TopKPooling(embedding_size, ratio=0.5)\n",
    "\n",
    "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform3 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn3 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool3 = TopKPooling(embedding_size, ratio=0.2)\n",
    "\n",
    "        # Final projection\n",
    "        self.linear1 = nn.Linear(embedding_size * 2, 512)\n",
    "        self.linear2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First Block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(self.head_transform1(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Second Block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(self.head_transform2(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Third Block\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(self.head_transform3(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Combine pooled features\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        # Final layers\n",
    "        x = self.linear1(x).relu()\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        graph_embedding = self.linear2(x)\n",
    "\n",
    "        return graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc83d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echtes Embedding Shape: torch.Size([1, 256])\n",
      "tensor([[-0.0012,  0.0287, -0.0383, -0.0205,  0.0359, -0.0504, -0.0081,  0.0101,\n",
      "         -0.0466,  0.0271, -0.0150,  0.0136, -0.0172,  0.0354, -0.0151, -0.0158,\n",
      "          0.0106, -0.0343,  0.0383, -0.0310,  0.0229,  0.0041, -0.0144,  0.0159,\n",
      "          0.0294,  0.0265, -0.0489,  0.0140,  0.0096,  0.0385, -0.0157,  0.0044,\n",
      "          0.0420,  0.0287, -0.0119,  0.0313,  0.0144,  0.0081, -0.0035,  0.0197,\n",
      "         -0.0032,  0.0357, -0.0419, -0.0226,  0.0123, -0.0388,  0.0438, -0.0119,\n",
      "         -0.0273, -0.0189,  0.0179,  0.0180,  0.0099,  0.0300,  0.0194,  0.0073,\n",
      "         -0.0089, -0.0176, -0.0224, -0.0379,  0.0397,  0.0220, -0.0062, -0.0254,\n",
      "          0.0340, -0.0484, -0.0065,  0.0296, -0.0079,  0.0006, -0.0468,  0.0034,\n",
      "         -0.0219,  0.0361,  0.0011,  0.0002, -0.0355, -0.0314,  0.0272,  0.0355,\n",
      "          0.0253,  0.0509, -0.0271, -0.0354, -0.0239, -0.0430,  0.0208, -0.0206,\n",
      "          0.0351,  0.0190,  0.0282, -0.0193, -0.0428, -0.0007, -0.0119, -0.0027,\n",
      "         -0.0197,  0.0211,  0.0315, -0.0048,  0.0332,  0.0110, -0.0167, -0.0295,\n",
      "         -0.0259,  0.0116, -0.0328, -0.0207, -0.0038,  0.0053, -0.0130,  0.0186,\n",
      "         -0.0482,  0.0156, -0.0389,  0.0349,  0.0050, -0.0267,  0.0377,  0.0235,\n",
      "         -0.0117,  0.0272,  0.0257, -0.0077, -0.0082,  0.0341,  0.0050, -0.0295,\n",
      "          0.0185,  0.0395, -0.0122, -0.0148, -0.0178, -0.0391, -0.0224,  0.0454,\n",
      "         -0.0421,  0.0011,  0.0382,  0.0440,  0.0370,  0.0118,  0.0397,  0.0017,\n",
      "         -0.0041, -0.0290, -0.0269, -0.0271,  0.0034, -0.0208, -0.0245, -0.0154,\n",
      "         -0.0314, -0.0509, -0.0532,  0.0089,  0.0125,  0.0349,  0.0083, -0.0344,\n",
      "         -0.0237,  0.0252,  0.0185, -0.0268,  0.0157,  0.0066, -0.0130,  0.0016,\n",
      "          0.0074, -0.0337, -0.0273,  0.0334, -0.0146,  0.0045, -0.0383,  0.0386,\n",
      "         -0.0257,  0.0352, -0.0016, -0.0271,  0.0206, -0.0085,  0.0208, -0.0428,\n",
      "          0.0370, -0.0133, -0.0078, -0.0197, -0.0334,  0.0232, -0.0289, -0.0165,\n",
      "         -0.0124, -0.0174, -0.0159,  0.0376, -0.0433,  0.0142,  0.0110, -0.0310,\n",
      "         -0.0292,  0.0009,  0.0318, -0.0035,  0.0094,  0.0132,  0.0352, -0.0418,\n",
      "          0.0006, -0.0149, -0.0491,  0.0021, -0.0305, -0.0299, -0.0264,  0.0066,\n",
      "         -0.0206,  0.0239,  0.0138, -0.0449, -0.0241,  0.0357,  0.0066, -0.0127,\n",
      "         -0.0251, -0.0355,  0.0356, -0.0200, -0.0037, -0.0505, -0.0353,  0.0210,\n",
      "          0.0385, -0.0135, -0.0215,  0.0355,  0.0419,  0.0074, -0.0039, -0.0354,\n",
      "         -0.0315, -0.0021, -0.0028,  0.0051,  0.0010,  0.0259,  0.0132, -0.0316,\n",
      "          0.0181,  0.0318,  0.0056,  0.0296, -0.0277,  0.0117,  0.0277,  0.0287]])\n"
     ]
    }
   ],
   "source": [
    "# Hole einen echten Datensatz\n",
    "data = dataset[0]  # Dataset[Index] gibt ein Data-Objekt zurück\n",
    "\n",
    "encoder = GNNEncoder(feature_size=3, embedding_size=512, output_dim=256)\n",
    "\n",
    "# Encoder aufrufen\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    embedding = encoder(data)\n",
    "\n",
    "print(\"Echtes Embedding Shape:\", embedding.shape)\n",
    "print(embedding)\n",
    "\n",
    "# Output: Gewichte in einem latenten Raum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
