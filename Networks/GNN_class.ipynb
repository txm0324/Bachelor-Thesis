{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9803ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torch_geometric.data import Dataset, Data # Installieren\n",
    "import os\n",
    "import numpy as np\n",
    "import torch # Installieren\n",
    "import scipy.sparse # Installieren\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gseapy as gp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d462e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_gmt = gp.parser.get_library('KEGG_2021_Human', organism='Human', min_size=3, max_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c016f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create files for each list\n",
    "gdsc_dataset = pd.read_csv('/sybig/home/tmu/TUGDA/data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:] \n",
    "pathway_list = list(kegg_gmt.keys())\n",
    "expression_data = gdsc_dataset.iloc[:, :1780]\n",
    "\n",
    "# combine 3-Fold-Validation Files\n",
    "response_1 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k1.csv\", index_col=0)\n",
    "response_2 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k2.csv\", index_col=0)\n",
    "response_3 = pd.read_csv(\"/sybig/home/tmu/TUGDA/data/cl_y_test_o_k3.csv\", index_col=0)\n",
    "response_data = pd.concat([response_1, response_2, response_3], axis=0, ignore_index=False)\n",
    "\n",
    "expression_data = expression_data.sort_index()\n",
    "response_data = response_data.sort_index()\n",
    "labels_df = response_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e948bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_data = expression_data[~expression_data.index.duplicated(keep='first')]\n",
    "labels_df = labels_df[~labels_df.index.duplicated(keep='first')]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugNetworkDataset(Dataset):\n",
    "    def __init__(self, root, drug_list, gene_list, pathway_list, labels_df, expression_data, transform=None, pre_transform=None):\n",
    "        self.drug_list = drug_list\n",
    "        self.gene_list = gene_list\n",
    "        self.pathway_list = pathway_list\n",
    "        self.labels_df = labels_df\n",
    "        self.expression_data = expression_data\n",
    "\n",
    "        # Entferne Duplikate im Index\n",
    "        self.expression_data = self.expression_data[~self.expression_data.index.duplicated(keep='first')]\n",
    "        self.labels_df = self.labels_df[~self.labels_df.index.duplicated(keep='first')]\n",
    "\n",
    "        # Alle Kombinationen aus Drug + Cell Line\n",
    "        self.samples = [\n",
    "            (drug, cell_line) \n",
    "            for drug in self.drug_list \n",
    "            for cell_line in self.labels_df.index\n",
    "        ]\n",
    "\n",
    "        self.custom_raw_dir = os.path.join(root, 'drug_matrices_csv')\n",
    "\n",
    "        self.graph_cache = {}\n",
    "\n",
    "        super(DrugNetworkDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f\"{drug}_matrix.csv\" for drug in self.drug_list]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return self.custom_raw_dir\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'drug_{drug}_cellline_{cell_line}.pt' for drug, cell_line in self.samples]\n",
    "\n",
    "    def download(self):\n",
    "        pass  # Nicht benötigt\n",
    "\n",
    "    def _load_adjacency_matrix(self, drug):\n",
    "        \"\"\"Lädt die Adjazenzmatrix für ein Drug\"\"\"\n",
    "        csv_path = os.path.join(self.raw_dir, f\"{drug}_matrix.csv\")\n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        return df\n",
    "\n",
    "    def _get_node_features(self, nodes, cell_line):\n",
    "        x = []\n",
    "        for node in nodes:\n",
    "            if node in self.gene_list:\n",
    "                 # If the node is a gene, get its expression value for the given cell line and type [expr, is_gene, is_pathway]\n",
    "                expr_value = self.expression_data.loc[cell_line, node]\n",
    "                x.append([float(expr_value), 1.0, 0.0])  # [expr, is_gene, is_pathway]\n",
    "            elif node in self.pathway_list:\n",
    "                # If the node is a pathway, use a default feature value of 0.0\n",
    "                x.append([0.0, 0.0, 1.0])  # [expr_dummy, is_gene, is_pathway]\n",
    "            else:\n",
    "                # Unknown node\n",
    "                x.append([0.0, 0.0, 0.0])  # [expr_dummy, is_gene, is_pathway]\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def get_node_name_by_index(self, idx, node_index):\n",
    "        drug, cell_line = self.samples[idx]\n",
    "        \n",
    "        # Lade die Adjazenzmatrix → gibt dir die Knotenliste\n",
    "        adj_df = self._load_adjacency_matrix(drug)\n",
    "        nodes = adj_df.columns.tolist()\n",
    "        \n",
    "        if node_index < len(nodes):\n",
    "            return nodes[node_index]\n",
    "        else:\n",
    "            raise IndexError(f\"Node index {node_index} out of range for this graph.\")\n",
    "    \n",
    "    def _load_graph(self, drug):\n",
    "        \"\"\"Lädt oder nutzt gecachten Graphen für ein Drug\"\"\"\n",
    "        if drug in self.graph_cache:\n",
    "            return self.graph_cache[drug]\n",
    "\n",
    "        csv_path = os.path.join(self.raw_dir, f\"{drug}_matrix.csv\")\n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        nodes = df.columns.tolist()\n",
    "        adj_matrix = df.values\n",
    "\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(adj_matrix)), dtype=torch.long)\n",
    "\n",
    "        # Merke dir den Graphen für dieses Drug\n",
    "        graph_data = {\n",
    "            \"nodes\": nodes,\n",
    "            \"edge_index\": edge_index\n",
    "        }\n",
    "\n",
    "        self.graph_cache[drug] = graph_data\n",
    "        return graph_data\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get(self, idx):\n",
    "        drug, cell_line = self.samples[idx]\n",
    "\n",
    "        # 1. Hole den Graphen für das Drug (entweder cached oder neu geladen)\n",
    "        graph_data = self._load_graph(drug)\n",
    "        nodes = graph_data[\"nodes\"]\n",
    "        edge_index = graph_data[\"edge_index\"]\n",
    "\n",
    "        # 2. Node Features (ändern sich je nach Zelllinie)\n",
    "        x = self._get_node_features(nodes, cell_line)\n",
    "\n",
    "        # 3. Label\n",
    "        y = torch.tensor([self.labels_df.loc[cell_line, drug]], dtype=torch.float)\n",
    "\n",
    "        # 4. Erstelle Data-Objekt\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, drug=drug, cell_line=cell_line)\n",
    "        data.nodes = nodes  # Optional: um Namen zu speichern\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        print(f\"Processing dataset: {len(self.samples)} graphs to save\")\n",
    "        for idx, (drug, cell_line) in tqdm(enumerate(self.samples), total=len(self.samples), desc=\"Processing Graphs\"):\n",
    "            filename = f'drug_{drug}_cellline_{cell_line}.pt'\n",
    "            pt_path = os.path.join(self.processed_dir, filename)\n",
    "\n",
    "            if not os.path.exists(pt_path):\n",
    "                data = self.get(idx)\n",
    "                torch.save(data, pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b39ccf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: 160800 graphs to save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Graphs:   7%|▋         | 11878/160800 [03:30<43:57, 56.47it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mDrugNetworkDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results/Network/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrug_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrug_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgene_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgene_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpathway_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpathway_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpression_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpression_data\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mDrugNetworkDataset.__init__\u001b[39m\u001b[34m(self, root, drug_list, gene_list, pathway_list, labels_df, expression_data, transform, pre_transform)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.custom_raw_dir = os.path.join(root, \u001b[33m'\u001b[39m\u001b[33mdrug_matrices_csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.graph_cache = {}\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDrugNetworkDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch_geometric/data/dataset.py:115\u001b[39m, in \u001b[36mDataset.__init__\u001b[39m\u001b[34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m._download()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_process:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch_geometric/data/dataset.py:262\u001b[39m, in \u001b[36mDataset._process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mProcessing...\u001b[39m\u001b[33m'\u001b[39m, file=sys.stderr)\n\u001b[32m    261\u001b[39m fs.makedirs(\u001b[38;5;28mself\u001b[39m.processed_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m path = osp.join(\u001b[38;5;28mself\u001b[39m.processed_dir, \u001b[33m'\u001b[39m\u001b[33mpre_transform.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    265\u001b[39m fs.torch_save(_repr(\u001b[38;5;28mself\u001b[39m.pre_transform), path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mDrugNetworkDataset.process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m pt_path = os.path.join(\u001b[38;5;28mself\u001b[39m.processed_dir, filename)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pt_path):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     torch.save(data, pt_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mDrugNetworkDataset.get\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    104\u001b[39m edge_index = graph_data[\u001b[33m\"\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# 2. Node Features (ändern sich je nach Zelllinie)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_line\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# 3. Label\u001b[39;00m\n\u001b[32m    110\u001b[39m y = torch.tensor([\u001b[38;5;28mself\u001b[39m.labels_df.loc[cell_line, drug]], dtype=torch.float)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mDrugNetworkDataset._get_node_features\u001b[39m\u001b[34m(self, nodes, cell_line)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gene_list:\n\u001b[32m     51\u001b[39m          \u001b[38;5;66;03m# If the node is a gene, get its expression value for the given cell line and type [expr, is_gene, is_pathway]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         expr_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpression_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcell_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m         x.append([\u001b[38;5;28mfloat\u001b[39m(expr_value), \u001b[32m1.0\u001b[39m, \u001b[32m0.0\u001b[39m])  \u001b[38;5;66;03m# [expr, is_gene, is_pathway]\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pathway_list:\n\u001b[32m     55\u001b[39m         \u001b[38;5;66;03m# If the node is a pathway, use a default feature value of 0.0\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/core/indexing.py:1182\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1180\u001b[39m key = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m is_iterator(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m   1181\u001b[39m key = \u001b[38;5;28mtuple\u001b[39m(com.apply_if_callable(x, \u001b[38;5;28mself\u001b[39m.obj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_is_scalar_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_tuple(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/core/indexing.py:1264\u001b[39m, in \u001b[36m_LocIndexer._is_scalar_access\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) != \u001b[38;5;28mself\u001b[39m.ndim:\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(k):\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = DrugNetworkDataset(\n",
    "    root=\"./results/Network/\",\n",
    "    drug_list=drug_list,\n",
    "    gene_list=gene_list,\n",
    "    pathway_list=pathway_list,\n",
    "    labels_df=labels_df, \n",
    "    expression_data=expression_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e055da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Name: Camptothecin\n",
      "Cell Line Name: 22RV1\n",
      "Edge Index:\n",
      "tensor([[   0,    0],\n",
      "        [   0,    8],\n",
      "        [   0,    9],\n",
      "        ...,\n",
      "        [1368, 1368],\n",
      "        [1369,  420],\n",
      "        [1369, 1369]])\n",
      "torch.Size([23662, 2])\n",
      "\n",
      "Node Features (x):\n",
      "tensor([[-0.8910,  1.0000,  0.0000],\n",
      "        [ 0.5178,  1.0000,  0.0000],\n",
      "        [-0.5985,  1.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1359,  1.0000,  0.0000],\n",
      "        [ 0.6368,  1.0000,  0.0000],\n",
      "        [ 0.6063,  1.0000,  0.0000]])\n",
      "torch.Size([1370, 3])\n",
      "Index 0: ABL1 → Feature: -0.8910\n",
      "Index 1: ACVR1B → Feature: 0.5178\n",
      "Index 2: ADORA1 → Feature: -0.5985\n",
      "Index 3: AR → Feature: 4.2872\n",
      "Index 4: ATF4 → Feature: -0.1862\n",
      "Index 5: ATM → Feature: -0.3401\n",
      "Index 6: ATR → Feature: -0.1959\n",
      "Index 7: AURKA → Feature: -0.3216\n",
      "Index 8: BAX → Feature: -0.6138\n",
      "Index 9: BBC3 → Feature: 0.4687\n",
      "\n",
      "Label (y):\n",
      "tensor([-3.1426])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "print(\"Drug Name:\", data.drug)\n",
    "print(\"Cell Line Name:\", data.cell_line)\n",
    "\n",
    "print(\"Edge Index:\")\n",
    "print(data.edge_index.t())\n",
    "print(data.edge_index.t().shape)\n",
    "\n",
    "print(\"\\nNode Features (x):\")\n",
    "print(data.x)\n",
    "print(data.x.shape)\n",
    "\n",
    "nodes = data.nodes\n",
    "\n",
    "for i in range(10):\n",
    "    node_name = nodes[i]\n",
    "    feature_value = data.x[i][0].item()  # Just take the first feature (gene expression)\n",
    "    print(f\"Index {i}: {node_name} → Feature: {feature_value:.4f}\")\n",
    "\n",
    "print(\"\\nLabel (y):\")\n",
    "print(data.y)\n",
    "print(data.y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4c928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Name: Camptothecin\n",
      "Cell Line Name: 23132-87\n",
      "Edge Index:\n",
      "tensor([[   0,    0],\n",
      "        [   0,    8],\n",
      "        [   0,    9],\n",
      "        ...,\n",
      "        [1368, 1368],\n",
      "        [1369,  420],\n",
      "        [1369, 1369]])\n",
      "torch.Size([23662, 2])\n",
      "\n",
      "Node Features (x):\n",
      "tensor([[-1.5698,  1.0000,  0.0000],\n",
      "        [ 0.6585,  1.0000,  0.0000],\n",
      "        [-0.5985,  1.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.4112,  1.0000,  0.0000],\n",
      "        [ 0.8274,  1.0000,  0.0000],\n",
      "        [ 0.0980,  1.0000,  0.0000]])\n",
      "torch.Size([1370, 3])\n",
      "Index 0: ABL1 → Feature: -1.5698\n",
      "Index 1: ACVR1B → Feature: 0.6585\n",
      "Index 2: ADORA1 → Feature: -0.5985\n",
      "Index 3: AR → Feature: -0.4920\n",
      "Index 4: ATF4 → Feature: 0.8800\n",
      "Index 5: ATM → Feature: 0.0323\n",
      "Index 6: ATR → Feature: 0.2790\n",
      "Index 7: AURKA → Feature: 1.0195\n",
      "Index 8: BAX → Feature: -0.2325\n",
      "Index 9: BBC3 → Feature: 0.6691\n",
      "\n",
      "Label (y):\n",
      "tensor([-3.2769])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[1]\n",
    "\n",
    "print(\"Drug Name:\", data.drug)\n",
    "print(\"Cell Line Name:\", data.cell_line)\n",
    "\n",
    "print(\"Edge Index:\")\n",
    "print(data.edge_index.t())\n",
    "print(data.edge_index.t().shape)\n",
    "\n",
    "print(\"\\nNode Features (x):\")\n",
    "print(data.x)\n",
    "print(data.x.shape)\n",
    "\n",
    "nodes = data.nodes\n",
    "\n",
    "for i in range(10):\n",
    "    node_name = nodes[i]\n",
    "    feature_value = data.x[i][0].item()  # Just take the first feature (gene expression)\n",
    "    print(f\"Index {i}: {node_name} → Feature: {feature_value:.4f}\")\n",
    "\n",
    "print(\"\\nLabel (y):\")\n",
    "print(data.y)\n",
    "print(data.y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl Samples:\", len(dataset))\n",
    "data = dataset[0]\n",
    "print(\"Node Features Shape:\", data.x.shape)\n",
    "print(\"Edge Index Shape:\", data.edge_index.shape)\n",
    "print(\"Label:\", data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd73cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ABL1\n",
      "22RV1    -0.890968\n",
      "23132-87 -1.569799\n",
      "42-MG-BA  0.536505\n",
      "5637      0.873951\n",
      "639-V    -0.708590\n",
      "...            ...\n",
      "YAPC      0.712502\n",
      "YH-13     1.926564\n",
      "YT       -2.203333\n",
      "ZR-75-30 -0.708590\n",
      "huH-1    -0.890968\n",
      "\n",
      "[804 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(expression_data[[\"ABL1\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f1d9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 8  # Je nach Graph-Größe und GPU-Speicher anpassen\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba14c295",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a GCN model structure that contains two GCNConv layers relu activation and a dropout rate of 0.5. The model consists of 16 hidden channels.  \n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Modell definieren\n",
    "# ----------------------------\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = torch.dropout(x, p=0.5, train=self.training)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "model = GCNModel(num_features=3, hidden_channels=16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26ef3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data_batch in tqdm(train_loader):  # Iteriere über Batches\n",
    "        data_batch = data_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_batch.x, data_batch.edge_index, data_batch.batch)\n",
    "        loss = criterion(out.squeeze(), data_batch.y)  # Verwende Label des gesamten Graphen\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data_batch.num_graphs\n",
    "\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data_batch in tqdm(loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "            out = model(data_batch.x, data_batch.edge_index, data_batch.batch)\n",
    "            loss = criterion(out.squeeze(), data_batch.y)\n",
    "            total_loss += loss.item() * data_batch.num_graphs\n",
    "    return total_loss / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6cb9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/20100 [00:45<6:17:21,  1.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m101\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m model.train()\n\u001b[32m      3\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Iteriere über Batches\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch_geometric/data/dataset.py:291\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03mpresent).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np.integer))\n\u001b[32m    288\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx.dim() == \u001b[32m0\u001b[39m)\n\u001b[32m    289\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m np.isscalar(idx))):\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     data = data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(data)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mDrugNetworkDataset.get\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     76\u001b[39m drug, cell_line = \u001b[38;5;28mself\u001b[39m.samples[idx]\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 1. Adjazenzmatrix laden\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m adj_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_adjacency_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m nodes = adj_df.columns.tolist()\n\u001b[32m     81\u001b[39m adj_matrix = adj_df.values\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mDrugNetworkDataset._load_adjacency_matrix\u001b[39m\u001b[34m(self, drug)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lädt die Adjazenzmatrix für ein Drug\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m csv_path = os.path.join(\u001b[38;5;28mself\u001b[39m.raw_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrug\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_matrix.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:334\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17561acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data_example = dataset[0].to(device)\n",
    "    pred = model(data_example.x, data_example.edge_index, data_example.batch)\n",
    "    print(\"Prediction:\", pred.item())\n",
    "    print(\"True Value:\", data_example.y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db522ce8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3badf18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1370, 3], edge_index=[2, 23662], y=[1], drug='Camptothecin', cell_line='23132-87', nodes=[1370])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44c23e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.070937\n",
      "Epoch: 100, Loss: 0.001628\n",
      "Prediction: -3.2174384593963623\n",
      "True Value: -3.276945114135742\n"
     ]
    }
   ],
   "source": [
    " # Create a GCN model structure that contains two GCNConv layers relu activation and a dropout rate of 0.5. The model consists of 16 hidden channels.  \n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Modell definieren\n",
    "# ----------------------------\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = torch.dropout(x, p=0.5, train=self.training)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = GCNModel(num_features=3, hidden_channels=16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Training\n",
    "# ----------------------------\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    loss = criterion(out, data.y.unsqueeze(0))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.6f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(data.x, data.edge_index, data.batch)\n",
    "    print(\"Prediction:\", pred.item())\n",
    "    print(\"True Value:\", data.y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70859eb8",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51fb5c",
   "metadata": {},
   "source": [
    "Komponenten: \n",
    "- GNNEncoder: Drug-Specific Network Graphen \n",
    "- Aim: tugda_mtl verarbeitet Rohdaten aus Expressionsprofilen --> Drug-spezfische Graphen aus DrugNetworkDataset verwenden, GNN-Encoder als Feature Extractor einbauen \n",
    "\n",
    "- Wir ersetzen:\n",
    "- Die bisherigen nn.Linear-Schichten durch den GNNEncoder. Die Eingabe von X_train durch drug_data (vom Typ Data). Der Output des Encoders wird an die S, A etc. Schichten weitergeleitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37a1cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, feature_size=1, embedding_size=512, output_dim=256):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "\n",
    "        # GNN Layers\n",
    "        self.conv1 = GATConv(feature_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform1 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool1 = TopKPooling(embedding_size, ratio=0.8)\n",
    "\n",
    "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform2 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool2 = TopKPooling(embedding_size, ratio=0.5)\n",
    "\n",
    "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.6)\n",
    "        self.head_transform3 = nn.Linear(embedding_size * 3, embedding_size)\n",
    "        self.bn3 = nn.BatchNorm1d(embedding_size)\n",
    "        self.pool3 = TopKPooling(embedding_size, ratio=0.2)\n",
    "\n",
    "        # Final projection\n",
    "        self.linear1 = nn.Linear(embedding_size * 2, 512)\n",
    "        self.linear2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First Block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(self.head_transform1(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Second Block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(self.head_transform2(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Third Block\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(self.head_transform3(x))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        # Combine pooled features\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        # Final layers\n",
    "        x = self.linear1(x).relu()\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        graph_embedding = self.linear2(x)\n",
    "\n",
    "        return graph_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc83d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echtes Embedding Shape: torch.Size([1, 256])\n",
      "tensor([[-0.0340,  0.0056,  0.0333, -0.0014,  0.0465, -0.0045, -0.0033, -0.0056,\n",
      "         -0.0343, -0.0003, -0.0384,  0.0118,  0.0132,  0.0353, -0.0043,  0.0051,\n",
      "         -0.0179,  0.0193, -0.0360, -0.0123,  0.0289,  0.0181, -0.0017, -0.0368,\n",
      "         -0.0129,  0.0491, -0.0041, -0.0287, -0.0133,  0.0006,  0.0312, -0.0048,\n",
      "         -0.0107, -0.0153,  0.0420,  0.0122,  0.0162, -0.0282, -0.0230, -0.0037,\n",
      "         -0.0423, -0.0312,  0.0334,  0.0155,  0.0018, -0.0340, -0.0045,  0.0048,\n",
      "         -0.0317,  0.0129, -0.0150, -0.0003, -0.0010,  0.0195,  0.0010,  0.0294,\n",
      "         -0.0101,  0.0239,  0.0062, -0.0011,  0.0234, -0.0264,  0.0053, -0.0193,\n",
      "          0.0415, -0.0296,  0.0008, -0.0174,  0.0538, -0.0461, -0.0371,  0.0447,\n",
      "         -0.0331, -0.0163, -0.0219,  0.0326,  0.0300,  0.0454, -0.0077, -0.0094,\n",
      "         -0.0374, -0.0422,  0.0356,  0.0216, -0.0058, -0.0117, -0.0180,  0.0456,\n",
      "          0.0244, -0.0266,  0.0173,  0.0364, -0.0243,  0.0089, -0.0180, -0.0324,\n",
      "         -0.0232, -0.0037,  0.0304,  0.0361, -0.0201, -0.0326,  0.0238,  0.0150,\n",
      "         -0.0243,  0.0104, -0.0054,  0.0013,  0.0015,  0.0482,  0.0155,  0.0016,\n",
      "          0.0123,  0.0074,  0.0231,  0.0141, -0.0419, -0.0267, -0.0077,  0.0044,\n",
      "         -0.0512,  0.0189,  0.0132, -0.0174,  0.0164,  0.0093, -0.0238,  0.0071,\n",
      "          0.0248,  0.0300,  0.0344,  0.0064, -0.0126,  0.0410, -0.0356, -0.0290,\n",
      "          0.0416, -0.0425,  0.0373,  0.0055, -0.0108, -0.0373, -0.0387, -0.0233,\n",
      "          0.0264, -0.0359, -0.0269,  0.0409,  0.0192,  0.0507,  0.0070, -0.0136,\n",
      "         -0.0343,  0.0159,  0.0427, -0.0240, -0.0353,  0.0396, -0.0525, -0.0100,\n",
      "          0.0235,  0.0033, -0.0354,  0.0310,  0.0288,  0.0087, -0.0157, -0.0037,\n",
      "          0.0304, -0.0010,  0.0364,  0.0101, -0.0213,  0.0122,  0.0304, -0.0383,\n",
      "         -0.0278,  0.0311,  0.0033, -0.0382,  0.0270,  0.0213,  0.0469, -0.0341,\n",
      "          0.0047, -0.0174, -0.0243, -0.0170, -0.0008, -0.0324,  0.0409, -0.0462,\n",
      "         -0.0001, -0.0213,  0.0081,  0.0265, -0.0222,  0.0065, -0.0282, -0.0184,\n",
      "          0.0339,  0.0517, -0.0020,  0.0199, -0.0046, -0.0416,  0.0209, -0.0082,\n",
      "          0.0352,  0.0229,  0.0405, -0.0148,  0.0071,  0.0133,  0.0057,  0.0279,\n",
      "         -0.0084, -0.0471, -0.0013, -0.0128,  0.0034,  0.0412, -0.0137, -0.0109,\n",
      "         -0.0088,  0.0298,  0.0186,  0.0201,  0.0274, -0.0167, -0.0273, -0.0271,\n",
      "         -0.0255,  0.0002,  0.0067, -0.0376,  0.0096, -0.0042,  0.0211, -0.0203,\n",
      "          0.0011, -0.0353,  0.0255,  0.0412,  0.0260,  0.0134, -0.0284,  0.0131,\n",
      "         -0.0084, -0.0088, -0.0286, -0.0033,  0.0106,  0.0454,  0.0056, -0.0280]])\n"
     ]
    }
   ],
   "source": [
    "# Hole einen echten Datensatz\n",
    "data = dataset[0]  # Dataset[Index] gibt ein Data-Objekt zurück\n",
    "\n",
    "encoder = GNNEncoder(feature_size=1, embedding_size=512, output_dim=256)\n",
    "\n",
    "# Encoder aufrufen\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    embedding = encoder(data)\n",
    "\n",
    "print(\"Echtes Embedding Shape:\", embedding.shape)\n",
    "print(embedding)\n",
    "\n",
    "# Output: Gewichte in einem latenten Raum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937a64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823c930e",
   "metadata": {},
   "source": [
    "# Cell Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e19c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_EmbedNet(nn.Module):\n",
    "    def __init__(self, fc_in_dim=1780, fc_hid_dim=[512, 512], embed_dim=256, dropout=0.5):\n",
    "        super(Cell_EmbedNet, self).__init__()\n",
    "        self.fc_hid_dim = fc_hid_dim\n",
    "        self.fc = nn.Linear(fc_in_dim, self.fc_hid_dim[0])\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        self.classifier = nn.ModuleList()\n",
    "\n",
    "        for input_size, output_size in zip(self.fc_hid_dim, self.fc_hid_dim[1:]):\n",
    "            self.classifier.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_size, output_size),\n",
    "                    nn.BatchNorm1d(output_size),\n",
    "                    self.act,\n",
    "                    nn.Dropout(p=self.dropout)\n",
    "                )\n",
    "            )\n",
    "        self.fc2 = nn.Linear(self.fc_hid_dim[-1], embed_dim)\n",
    "\n",
    "        # Weight init\n",
    "        for layer in self.classifier:\n",
    "            nn.init.xavier_uniform_(layer[0].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        for fc in self.classifier:\n",
    "            x = fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6341297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugCellPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=256):\n",
    "        super(DrugCellPredictor, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, 1)  # Ausgabe: z.B. AUC-Wert\n",
    "        )\n",
    "\n",
    "    def forward(self, drug_emb, cell_emb):\n",
    "        combined = torch.cat([drug_emb, cell_emb], dim=1)  # shape: [batch_size, 512]\n",
    "        prediction = self.mlp(combined)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gnn_encoder = GNNEncoder(output_dim=256)\n",
    "        self.cell_encoder = Cell_EmbedNet(fc_in_dim=1780, embed_dim=256)\n",
    "        self.predictor = DrugCellPredictor(input_dim=512)\n",
    "\n",
    "    def forward(self, drug_data, cell_expression):\n",
    "        drug_emb = self.gnn_encoder(drug_data)\n",
    "        cell_emb = self.cell_encoder(cell_expression)\n",
    "        prediction = self.predictor(drug_emb, cell_emb)\n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
